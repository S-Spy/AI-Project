
Поле действий - карта
Взаимодействия:
	Доступные обьекты:
		турфы
			Действия: переход
		игрок
			Действия: *муха*


Лог уникальных слов приветствий и неприветствий с вероятностью напротив
Вероятность будет вычисляться статистически.




Как определить категорию слова?
Входные данные: Память, сигнал
Вместо того, чтобы определять комбинации букв, легче просто написать сами слова. Так что не вариант
Самое простое - выдать пользователю запрос на указание категории. Сделать как запасной вариант
Можно сравнить с имеющейся памятью на схожесть. Но это лишь укажет однокоренные слова максимум. Или засорит лог. На самый худой конец
Можно обучать постоянно. Задавать наводящие вопросы игрокам, все такое. Хороший вариант, но на самообучение похоже слабо

Как же самостоятельно определить категорию слова?
Человек выучивает новые слова как?
Получает на вход данные про значение слова из словаря или статистического преобладания в определенных ситуациях. Три раза минимум

То есть нужно определить ситуации
Искать по миру слова из памяти и анализировать ответы на них от других игроков.
Сработает только с однокатегорийным ответом вроде привет-привет.


Нужно не бояться спрашивать. Но и грузить игроков многочисленными запросами тоже не стоит.
Но это решится по мере заполнения БД

Анализ слова возможен только по схожести букв. Проверка на дурака не обязательна. Только наличие слов в выражении
Или по определенным ситуациям
Ситуации:
	Наводящие вопросы
	Прямой вопрос про категорию
	Поиск и анализ ситуаций с парными однокатегорийными обращениями-ответами










//////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////////////////
//------------------------------------------------------------------------------------------//
//							Алгоритм самообучающегося ИИ									//
//------------------------------------------------------------------------------------------//
//////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////////////////








Для упрощения будем считать, что нам заведомо известны количество слов в ответе и в вопросе. И на каждое из них дается по одному нейрону.

Изначально есть два этапа: вход и выход
Входом считается ряд нейронов-слов
Выходом считается ряд нейронов-ответных слов
То есть есть два ряда.

Для наглядности вот так



//////////////
			//
0 0 0 0 0  	//
			//
 1 1 1 1	//
 			//
//////////////



Если ограничиваться только этими двумя рядами, то в каждое ответное слово будет проведена связь с вопросным словом.

То есть каждый выход(1) связан с каждым входом(0), но между собой нейроны в рядах не связываются.

Исходя из этой простой двухрядной модели, в выходном нейроне будет проводиться очень сложный алгоритм по определению ответного слова
Порядок слов ответа по такой модели не вычисляется. Просто первый нейрон=первое слово
Выяснение порядка слов это целая отдельная задача, так что пока забудем о ней и будем довольствоваться мешаниной слов, хоть и пригодных как ответ, если их переставить местами
Алгоритм определения слова очень сложен, и чтобы упростить понимание, расширим нашу нейронную сеть
С учетом этих данных добавим еще пару рядов посередине.



//////////////
			//
0 0 0 0 0 	// - вопрос к чатботу (каждый 0 - слово в текстовом виде)
			//
 1 1 1 1	// - определяем тип слова для ответа.
			//
 2 2 2 2	// - определяем само слово исходя из типа(изменено)
 			//
//////////////



Связка линейная
{Все 0}->1->{Все 2}



//////////////
			//
0 0 0 0 0	//
\  \|/ /	//
    1		//
 / /|\		//
2 2 2 2		//
 			//
//////////////



Алгоритм упростился.

Теперь явно видно, что слово для ответа напрямую выбирается из вычисленного типа. А тип для всех слов сразу вычисляетчя в одном нейроне по сложному алгоритму исходя из слов в вопросе(изменено)
Тут же можно увидеть, что если увеличить количество нейронов, то есть, число передаваемых параметров в словах вопроса, то тип будет определяться точнее. Скажем, если передавать дополнтельно параметр вежливости(01 01 01 01 01), то тип ответа определить станет легче
Чем больше параметров в ряду, тем точнее и легче определяется следующий ряд
Так вот, алгоритм определения типа ответа все так же сложен. Для упрощения наглядности алгоритма добавим еще один ряд нейронов, в которых определяется тип слов в вопросе



//////////////
			//
0 0 0 0 0	// - В первом ряду просто слова вопроса
| | | | |	//
1 1 1 1 1	// - Во втором мы определяем явно простым методом тип каждого слова(количество нейронов в первом и втором ряду равно)
\ \ |/ /	//
    2		// - В третьем уже исходя из типа мы берем простым алгоритмом выбираем общий тип
 / /| \		//
3 3 3  3	// - В четвертом исходя из типа ответа мы подбираем текст слова
			//
//////////////




//-----------------------------------------------------------------------------------------------
//				Это обьясняет алгоритм, но не обьясняет механику обучения
//-----------------------------------------------------------------------------------------------





Допустим, мы имеем фиксированное количество слов в словаре

Насколько мы помним, пригодность слова опрелеляется по тегу(который тип). То есть слова в словаре связано с каждым типом и имеет параметр пригодности. Допустис, для типа "привет" пригодность равна 50%
И вот на этом параметре пригодности и будет основано все обучение

По текущей схеме параметр пригодности передается внутри нейрона 3 ряда. То есть слово определяется исходя из типа(получается из внешнего) и вычисляется линейным методом подбора из словаря.


//////////////////////////////////////////////////////////////////////
																	//
3:																	//
Слово = рандомное_слово_из_словаря(тип, минимальная_пригодность)	//
																	//
//////////////////////////////////////////////////////////////////////

Обучение заключается в том, что после того, как мы подобрали это слово, его пригодность для этого типа внутри словаря увеличивается на 5%, для примера
Если смысл входящего предложения - отрицание, агрессия или что то вроде в ответ на прошлые действия чатбота - то пригодность типа для прошлого ответа ИИ уменьшается на 10%(5 процентов чтобы отменть изменение и 5, чтобы уменьшить пригодность)
Суть нескольких нейронов для одного обьекта в том, что чем больше для вычислений используется алгоритмов, тем меньше потом придется проделывать манипуляций с этим нейроном в другом. Все четко и ясно и меньше шанс ошибки в написании алгоритма. И уж тем более легче разобраться в самом алгоритме, что позволяет сделать его более точным
Можно смешать и проделывать сразу гигантские мпхинации в одном нейроне, как я указал в начале, в двухрядной модели, но это сложно и можно легко упустить детали. Хотя теоретически, можно написать настолько же эффективный код. Только непонятный и запутанный, что не ценится наукой.(изменено)
Чтобы показать как определяется отрицание в ответ на действия ИИ, я усложню схему и добавлю не совсем линейное взаимодействие



//////////////////
				//
0 0 0 0 0  4	// - на входе слова вопроса и статус того, говорил ли ИИ раньше
| | | | |  |	//
1 1 1 1 1  |	// - сначала определяем тип слов вопроса
\  \|/ /   |	//
    2<----- 	// = если ИИ говорил раньше и это отрицание, то уменьшаем пригодность слов из прошлого ответа
    |			//
 / /|\			//
3 3 3 3			//
				//
//////////////////



На этой схеме видно, что для проверки отрицания мы перескочили через один ряд нейронов
Это обьясняется просто. В первом ряду находятся только входные данные, не вычисляемые. И они не обязательно должны использоваться в следующем ряду
Для полной информативности скажу, что эта схема подразумевает следующие процедуры, протекающие в нейронах



//////////////////////////////////////////////////////////////////
																//
2:																//
IF(тип_вопроса="отрицание" && статус_прошлого_ответа=TRUE)   	//
	уменьшить_пригодность_прошлого_ответа()						//
																//
3:																//
IF(тип_ответа="молчание")										//
	статус_прошлого_ответа=FALSE								//
ELSE															//
	статуспрошлого_ответа=TRUE									//
																//
//////////////////////////////////////////////////////////////////



Дальше можно долго расширять эту схему для расширения функционала.
Нас теперь интересует то, как подобрать нужное число нейронов, если мы решили их фиксировать.
Ответ: для данной роли - никак.

Поясняю. Дополнительные нейроны не будут нести никакого функционала, если мы не добавим параметр веса. Но это сложно.
Поэтому число нейронов в каждом ряду напрямую зависит от числа нейронов в предыдущем
То есть, если будет 10 слов в вопросе, то

В ряду входа будет 11 нейронов
2 ряд: 10 нейронов
3 ряд: 1 нейрон
4 ряд: не определено. Мы зафиксировали на 10

В 4 ряду не определено потому, что мы не знаем, как строить предложение для ответа. Так что ответ равен кучке подходящих для ответа слов в числе равном числу слов в вопросе
Фух, выговорился. Я только сейчас понял, что нейронная сеть это по сути просто особый способ рисования блок схем,, облегчающих понимание Кода Хаоса. Набор правил, если угодно
Можно использовать самодельную смесь, с таким же функционалом, но понимать её будет гораздо сложнее, потому что в ней не будет видно порядка
Эта схема использует аналоговые сигналы. Хотя рекомендуется стремиться к весовым. То есть, по хорошему, нейроны должны передавать не слова и названия типов, а только число в процентах от 0 до 100
То есть вероятность выбора типа слова "привет" определяется в одном нейроне, а вероятносит типа слова "пока" в соседнем. Весь этот ряд передается на следующий уровень, где вероятности складываются и вычисляется окончательный тип слова
Вот в такой схеме используется вес нейрона для регулирования его важности в вычислениях. Она позволяет использовать фиксированное число нейронов максимально эффективно



К примеру, в словаре 3000 типов. Мы выделили под ряд определения типов 9000 нейронов.
В итоге вероятность того, что тип слова равен "привет" будет вычисляться аж в 3 местах сразу!

Как это работает:
1. Нейрон вычисляет вероятность того, что слово такого типа
2. Помимо этого нейрон имеет свою важность(вес)
3. Все это перемножается и идет на следующий ряд
4. В следующем ряду для типа привет было получено 70%, 50% и 75%. Вычисляется среднее значение и вероятность того, что слово имеет тип "привет" равна 65%
5. Если это максимальная вероятность, то нейрон выдает ответ "привет"
6. Помимо этого увеличивает вес одного из трех нейронов для этого ответа на 5%. Почему только один? Я сам не знаю, можно и все три. Но смысла особо не вижу

По сути, это то же самое обучение, только теперь оно завязано не на отдельном параметре, а на весе нейрона


Зачем нужно увеличение числа нейронов ради одного ответа? То есть аж три нейрона одинаково вычисляющих вероятность типа "привет"
Алгоритмы вычисления действуют по одной схеме. Погрешность нейрона может быть довольно высока. Скажем, один нейрон из-за особенностей алгоритма может выдать ответ только 60% или 65%. Погрешность не позволит ему выбрать число между 60 и 65. Можно использовать тяжелый алгоритм для конкретизации числа, завязанный на уникальных переменных, но это сделает нейрон неуниверсальным в потенциале. Если вдруг голова озарится идеей универсального обучения, то такой нейрон прекрасно расправится с вычислением 2+2, но с определением цвета картинки сильно накосячит. Поэтому используется легкая и простая функция среднего значения. Чем больше нейронов, тем точнее значение может быть
Хотя есть альтернативное применение. Их много. Я лишь пытаюсь понять и излагаю базовые методы, до которых я додумался
Например, дополнительные нейроны можно использовать для создания искусственной погрешности
Такие функции, как чувство одобрения или отрицания заложены организмом отдельно, как гормоны, поэтому их нельзя спрятать в универсальном нейронном коде. Они базис
Их можно использовать как отдельный блок переменный, меняющихся отдельно. К примеру, гормон счастья влияет на восприятие чужих слов. По аналогии с реальными гормонами

Хех... ИИ с химией мозга

Если ИИ до предела счастлив, то проста фраза "мне не нравится то что ты сказал" не будет считаться им как отрицание и реакция будет строиться соответствующе
ИИ, который, откровенно говоря, забухал и замкнулся в себе от депрессии через некоторое время сменит свои приоритеты на полное молчание

Все чаще он будет выбирать молчание как ответ и это станет его нейронным приоритетом. Только гормон счастья научит его снова говорить






//-----------------------------------------------------------------------------------------------

С учетом этого можно составить более полноценную нейронную модель. Правда, с нуля
Каждый нейрон обладает параметром W - вес, и name - аналоговый сигнал(текст). По сути, это означает сдвоенный нейрон.



//////////////////////////////
							//
{   0        0     	 ...}	// - на входе имеем очень длинный ряд нейронов, равный числу слов в словаре. У всех вес равен 1
	|		 |		  |		//
{1 1 ...} {1 1 ...}  ...	// - для каждого слова выделяется число нейронов, равное числу типов слов. Изначально вес равен 0
							//
//////////////////////////////



Так как дальше пойдет еще больше разветвлений, я вместо неопределенно
длинного ряда буду писать троеточие, и составлю граф только для одного слова в вопросе



//////////////////////////////
							//
{Q		...} 				// - Входящие слова из вопроса. Если слова нет в словаре, оно добавится после проверки всех нейронов в следующем ряду. Граф рассчитан на одно слово. Так будет для каждого из них
 |		 |					//
/-----------\				// ------------------------------------------------------------------------
 +			|				//
 | 			|				//
{0  0 ...}	|				// - Весь словарь для сверки со словами в вопросе. OQ.Weight = схожесть
 | /  /		|				//
 |			|				//
 1			|				// - Выбирается наиболее похожее слово. Если нет 100% схожести, то добавляется в словарь. Если при добавлении в словарь нет 1% схожести, то добавляется список типов с весом 50%. Иначе список копируется от максимально похожего слова
 |			|				//
 | \		|				//
{2 ...}		|				// - определяется вероятность каждого типа для данного слова
 | /		|				//
 |			|				//
 3			|				// - определяем тип слова в входящем сигнале
 |			|				//
 +			|				//
\-----------/				// ------------------------------------------------------------------------
 |	   /					//
 |	 /						//
 | /						//
 |							//
 4							// - составляется прототип
 | \  \						//
 | {5 ...}					// - сверяется c каждым из существующих типов
 | /  /						//
 6							// - определяется тип входящего сигнала и обновляется Q6.Weight и Q4.Weight. В случае отсутствия, прототип добавляется в словарь
 | \						//
 |   G						// - Эта фраза меняет гормональный фон(Тут же хранится статус ожидания ответа)
 | /						//
 7							// - вычисляем тип необходимого ответа исходя из типа входящего сигнала и гормонального фона. Если реакция негативна и это ответ, то уменьшаем вес используемых в прошлый раз нейронов-слов в словаре для типа
 | 							//
 |  \						//
{8  ...}					// - ряд из всего словаря
 |  /						//
 9							// - записываем в нейрон список пригодных слов
 |							//
 A							// - на основе этого списка составляется предложение по сложному алгоритму. Увеличиваем вес выбранного типа для выбранных слов
							//
//////////////////////////////



Если при сверке словаря слово не обнаружилось, то оно добавляется в список типов
(По умолчанию вес равен 50%) и в словарь
Данная модель хоть и жутко неоптимальна, но может обучаться новым словам и типам и выдавать мешанину слов, пригодных для данного типа

В идеале, для построения предложений можно составить универсальный нейронный алгоритм, способный выучить любой язык, для этого нужно использовать тип, который одновременно является и логом фраз.
Но как альтернативу, можно использовать сложный алгоритм, завязанный на прописанных вручную правилах языка(По нему целые учебники пишут, а я тут код планирую, ага)